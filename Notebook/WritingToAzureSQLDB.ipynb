{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Reading from and Writing to Azure SQL Database using Azure Machine Learning\r\n",
        "\r\n",
        "The purpose of this notebook is to demonstrate how to read from and write to Azure Machine Learning using the pipeline approach.  This assumes the following:\r\n",
        "* You have configured a Datastore named `expense_reports`.\r\n",
        "* You want to use the `workspaceblobstore` Azure Blob Storage Datastore.  You can, of course, create your own and substitute it.\r\n",
        "* You have configured an Azure Data Factory.\r\n",
        "\r\n",
        "First up, let's load some important Python libraries and obtain the workspace from our Azure ML config."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import azureml.core\r\n",
        "from azureml.core import Workspace, Datastore, Dataset\r\n",
        "from azureml.data.datapath import DataPath\r\n",
        "from azureml.data import DataType\r\n",
        "from azureml.pipeline.steps import DataTransferStep\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "ws = Workspace.from_config()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1616188113540
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieve the expenses and Blob Storage datastores from Azure Machine Learning."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "expenses_datastore = Datastore.get(ws, datastore_name=\"expense_reports\")\r\n",
        "blob_datastore = Datastore.get(ws, datastore_name=\"workspaceblobstore\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616185616652
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query our data.  This will hit the `expenses_datastore` Datastore and will pull back data prior to 2017.  Note that we need to specify the data types for each input in `from_sql_query`, but the resulting output of this is a Pandas DataFrame, making it easy to work with."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = DataPath(expenses_datastore, \"\"\"\r\n",
        "SELECT\r\n",
        "    er.EmployeeID,\r\n",
        "    CONCAT(e.FirstName, ' ', e.LastName) AS EmployeeName,\r\n",
        "    ec.ExpenseCategoryID,\r\n",
        "    ec.ExpenseCategory,\r\n",
        "    er.ExpenseDate,\r\n",
        "    YEAR(er.ExpenseDate) AS ExpenseYear,\r\n",
        "    -- Python requires FLOAT values--it does not support DECIMAL\r\n",
        "    CAST(er.Amount AS FLOAT) AS Amount\r\n",
        "FROM dbo.ExpenseReport er\r\n",
        "    INNER JOIN dbo.ExpenseCategory ec\r\n",
        "        ON er.ExpenseCategoryID = ec.ExpenseCategoryID\r\n",
        "    INNER JOIN dbo.Employee e\r\n",
        "        ON e.EmployeeID = er.EmployeeID\r\n",
        "WHERE\r\n",
        "    er.ExpenseDate < '2017-01-01'\"\"\")\r\n",
        "\r\n",
        "data_types = {\r\n",
        "    'EmployeeID': DataType.to_long(),\r\n",
        "    'EmployeeName': DataType.to_string(),\r\n",
        "    'ExpenseCategoryID': DataType.to_long(),\r\n",
        "    'ExpenseCategory': DataType.to_string(),\r\n",
        "    'ExpenseDate': DataType.to_datetime('%Y-%m-%d'),\r\n",
        "    'ExpenseYear': DataType.to_long(),\r\n",
        "    'Amount': DataType.to_float()\r\n",
        "}\r\n",
        "\r\n",
        "expense_reports = Dataset.Tabular.from_sql_query(query, set_column_types=data_types).to_pandas_dataframe()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616185778933
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This gives you a feel for what the data looks like.  It's fairly straightforward, tabular data.  Almost like I generated it for a demo or something."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "expense_reports.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616187280826
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the `RandomForestRegressor` from scikit-learn and fit for `Amount` given `ExpenseCategoryID` and `ExpenseYear`.  It's a simple model but it works pretty well."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\r\n",
        "reg = RandomForestRegressor() \r\n",
        "model = reg.fit(expense_reports[[\"ExpenseCategoryID\", \"ExpenseYear\"]], expense_reports[[\"Amount\"]].values.ravel())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616187285275
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's the data that we'll use for predictions.  This is expense reports from 2017 forward."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = DataPath(expenses_datastore, \"\"\"\r\n",
        "SELECT\r\n",
        "    er.EmployeeID,\r\n",
        "    CONCAT(e.FirstName, ' ', e.LastName) AS EmployeeName,\r\n",
        "    ec.ExpenseCategoryID,\r\n",
        "    ec.ExpenseCategory,\r\n",
        "    er.ExpenseDate,\r\n",
        "    YEAR(er.ExpenseDate) AS ExpenseYear,\r\n",
        "    -- Python requires FLOAT values--it does not support DECIMAL\r\n",
        "    CAST(er.Amount AS FLOAT) AS Amount\r\n",
        "FROM dbo.ExpenseReport er\r\n",
        "    INNER JOIN dbo.ExpenseCategory ec\r\n",
        "        ON er.ExpenseCategoryID = ec.ExpenseCategoryID\r\n",
        "    INNER JOIN dbo.Employee e\r\n",
        "        ON e.EmployeeID = er.EmployeeID\r\n",
        "WHERE\r\n",
        "    er.ExpenseDate >= '2017-01-01'\"\"\")\r\n",
        "\r\n",
        "data_types = {\r\n",
        "    'EmployeeID': DataType.to_long(),\r\n",
        "    'EmployeeName': DataType.to_string(),\r\n",
        "    'ExpenseCategoryID': DataType.to_long(),\r\n",
        "    'ExpenseCategory': DataType.to_string(),\r\n",
        "    'ExpenseDate': DataType.to_datetime('%Y-%m-%d'),\r\n",
        "    'ExpenseYear': DataType.to_long(),\r\n",
        "    'Amount': DataType.to_float()\r\n",
        "}\r\n",
        "\r\n",
        "expense_reports_to_predict = Dataset.Tabular.from_sql_query(query, set_column_types=data_types).to_pandas_dataframe()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616187313008
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our data, generate predictions.  Then, concatenate the `PredictedAmount` column onto the expense reports DataFrame so that we can see the inputs as well as the prediction."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred = pd.DataFrame({\"PredictedAmount\" : model.predict(expense_reports_to_predict[[\"ExpenseCategoryID\", \"ExpenseYear\"]]) })\r\n",
        "output_data_set = pd.concat([expense_reports_to_predict, pred], axis=1)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616187315493
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a brief view of the resulting outputs."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_data_set.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616187317566
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I want to write the results to Azure SQL Database.  The thing is, though, that there's no direct way to perform that write.  The best available option (as of March of 2021) is to write the data to Azure Blob Storage and then transfer that data to Azure SQL Database.\r\n",
        "\r\n",
        "Well, to write the data to Azure Blob Storage, I first need to write it locally and then transfer.  I'll call the output `predictions.csv`."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "if not os.path.exists('data'):\r\n",
        "    os.mkdir('data')\r\n",
        "local_path = 'data/predictions.csv'\r\n",
        "output_data_set.to_csv(local_path, index=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616187331040
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the data is saved locally, we can upload it to Azure Blob Storage.  This is where I first use the `blob_datastore` Datastore.  I'm going to write it to `ExpenseReportPrediction/predictions.csv`.  If you go digging into the storage account Azure Machine Learning uses, you can find this folder."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blob_datastore.upload(src_dir='data', target_path='ExpenseReportPrediction')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616187333563
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want to load a few more objects in order to build out a pipeline.  Technically, we could have made training and inference steps in this pipeline as well, and that's what I'd do on a production project."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Experiment\r\n",
        "from azureml.pipeline.core import Pipeline\r\n",
        "from azureml.data.data_reference import DataReference\r\n",
        "from azureml.pipeline.steps import DataTransferStep\r\n",
        "from azureml.data.sql_data_reference import SqlDataReference\r\n",
        "from azureml.core.compute import ComputeTarget, DataFactoryCompute "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616187338800
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to bring in Azure Data Factory.  To do that, we specify the location of the data factory, both its resource group (`rg`) and the Data Factory name (`adf`)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rg = '<Resource Group>'\r\n",
        "adf='<Data Factory>'\r\n",
        "adfcompute = 'amlcompute-adf'\r\n",
        "\r\n",
        "adfconfig = DataFactoryCompute.attach_configuration(resource_group=rg, factory_name=adf, resource_id=None)\r\n",
        "adf_compute = ComputeTarget.attach(workspace=ws, name=adfcompute, attach_configuration=adfconfig)\r\n",
        "adf_compute.wait_for_completion()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616187540267
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a `DataReference()` reference to bring in Azure Blob Storage.  We'll read all of the files from `ExpenseReportPrediction/`."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_blob_ref = DataReference(\r\n",
        "    datastore=blob_datastore,\r\n",
        "    data_reference_name=\"prediction_blob_ref\",\r\n",
        "    path_on_datastore=\"ExpenseReportPrediction/\",\r\n",
        "    mode=\"mount\",\r\n",
        "    path_on_compute=None,\r\n",
        "    overwrite=False\r\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616187565403
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bring in Azure SQL Database as a `SqlDataReference`.  We will write out to the `ExpenseReportPrediction` table in SQL Server.  Note that this table must already exist prior to executing the pipeline!"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_sql_ref = SqlDataReference(\r\n",
        "    datastore=expenses_datastore,\r\n",
        "    data_reference_name=\"prediction_sql_ref\",\r\n",
        "    sql_table=\"ExpenseReportPrediction\",\r\n",
        "    sql_query=None,\r\n",
        "    sql_stored_procedure=None,\r\n",
        "    sql_stored_procedure_params=None\r\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616187567951
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This `DataTransferStep` migrates our data from Azure Blob Storage into Azure SQL Database.  We set `allow_reuse=False` here because that allows us to re-run the operation with new data (but the same code) and actually get results.  if `allow_reuse=True`, re-running this will return a completed status but not do anything new after the first time it runs."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transfer_blob_to_sql = DataTransferStep(\r\n",
        "    name=\"transfer_blob_to_sql\",\r\n",
        "    source_data_reference=prediction_blob_ref,\r\n",
        "    destination_data_reference=prediction_sql_ref,\r\n",
        "    compute_target=adf_compute,\r\n",
        "    allow_reuse=False,\r\n",
        "    destination_reference_type=None\r\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616187570661
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And here's the `Pipeline` which will do the work.  It has one step:  `transfer_blob_to_sql`."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datatransfer_pipeline = Pipeline(workspace=ws, \r\n",
        "    steps=[transfer_blob_to_sql], \r\n",
        "    description='Transfer blob data to sql')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616187574016
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We execute the pipeline in the context of an `Experiment`.  It takes a little while to execute and gives us a summary of what happened.  The net result here is that we inserted the data we wanted into `dbo.ExpenseReportPrediction`."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exp = Experiment(workspace = ws, name=\"DataTransfer_BlobtoSQL\")\r\n",
        "\r\n",
        "exp_pipelinerun = exp.submit(datatransfer_pipeline)\r\n",
        "\r\n",
        "exp_pipelinerun.wait_for_completion()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616187750411
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}