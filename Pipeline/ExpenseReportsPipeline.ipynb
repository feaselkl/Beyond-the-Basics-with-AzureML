{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Build a simple ML pipeline for expense report estimation\r\n",
    "\r\n",
    "## Introduction\r\n",
    "This tutorial shows how to train and deploy a model which we can use to estimate expense report expenditures.  The goal with this model is to create a service which we can call and get an estimation of how far off from reality our expense reports are.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up your development environment\r\n",
    "\r\n",
    "All the setup for your development work can be accomplished in a Python notebook.  Setup includes:\r\n",
    "\r\n",
    "### Import packages\r\n",
    "\r\n",
    "Import Python packages you need in this session. Also display the Azure Machine Learning SDK version.  If you have not already done so, you will need to ensure that [the Azure ML SDK is installed](https://docs.microsoft.com/en-us/python/api/overview/azure/ml/install?view=azure-ml-py).  If you are running this from outside an Azure ML compute instance, run the following operations to install the relevant SDK libraries.\r\n",
    "\r\n",
    "```python\r\n",
    "pip install azureml-core\r\n",
    "pip install azureml-pipeline\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "import azureml.core\r\n",
    "from azureml.core import (\r\n",
    "    Workspace,\r\n",
    "    Dataset,\r\n",
    "    Datastore,\r\n",
    "    ComputeTarget,\r\n",
    "    Experiment,\r\n",
    "    ScriptRunConfig,\r\n",
    ")\r\n",
    "from azureml.data import DataType, OutputFileDatasetConfig\r\n",
    "from azureml.data.datapath import DataPath\r\n",
    "from azureml.data.dataset_consumption_config import DatasetConsumptionConfig\r\n",
    "from azureml.pipeline.steps import PythonScriptStep\r\n",
    "from azureml.pipeline.core import Pipeline, PipelineParameter, PipelineRun\r\n",
    "\r\n",
    "# check core SDK version number\r\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ],
   "outputs": [],
   "metadata": {
    "gather": {
     "logged": 1618857515040
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Connect to workspace\r\n",
    "\r\n",
    "Create a workspace object from the existing workspace. `Workspace.from_config()` reads the file **config.json** and loads the details into an object named `workspace`.\r\n",
    "\r\n",
    "If you do not already have a **config.json** file, uncomment and run the first batch of code to create one."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#workspace = Workspace.get(name=\"<Enter your workspace name>\",\r\n",
    "#                subscription_id=\"<Enter your subscription ID>\",\r\n",
    "#                resource_group=\"<Enter your resource group>\")\r\n",
    "#workspace.write_config(file_name=\"config.json\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load workspace\r\n",
    "workspace = Workspace.from_config()\r\n",
    "print(\r\n",
    "    \"Workspace name: \" + workspace.name,\r\n",
    "    \"Azure region: \" + workspace.location,\r\n",
    "    \"Resource group: \" + workspace.resource_group,\r\n",
    "    sep=\"\\n\",\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "gather": {
     "logged": 1618857518344
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create experiment and a directory\n",
    "\n",
    "Create an experiment to track the runs in your workspace and a directory to deliver the necessary code from your computer to the remote resource."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create an ML experiment\r\n",
    "exp = Experiment(workspace=workspace, name=\"ExpenseReportsPipeline\")\r\n",
    "\r\n",
    "# create a directory for source data\r\n",
    "script_folder = \"./src\"\r\n",
    "os.makedirs(script_folder, exist_ok=True)\r\n",
    "\r\n",
    "# create a temp directory for data\r\n",
    "tmp_folder = \"./tmp\"\r\n",
    "os.makedirs(tmp_folder, exist_ok=True)"
   ],
   "outputs": [],
   "metadata": {
    "gather": {
     "logged": 1618857519697
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create or Attach existing compute resource\n",
    "\n",
    "**Creation of compute takes approximately 5 minutes.** If the AmlCompute with that name is already in your workspace the code will skip the creation process."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
    "from azureml.core.compute_target import ComputeTargetException\r\n",
    "\r\n",
    "# choose a name for your cluster\r\n",
    "cluster_name = \"cpu-cluster\"\r\n",
    "\r\n",
    "found = False\r\n",
    "# Check if this compute target already exists in the workspace.\r\n",
    "cts = workspace.compute_targets\r\n",
    "if cluster_name in cts and cts[cluster_name].type == \"AmlCompute\":\r\n",
    "    found = True\r\n",
    "    print(\"Found existing compute target.\")\r\n",
    "    compute_target = cts[cluster_name]\r\n",
    "if not found:\r\n",
    "    print(\"Creating a new compute target...\")\r\n",
    "    compute_config = AmlCompute.provisioning_configuration(\r\n",
    "        vm_size=\"STANDARD_D2_V2\",\r\n",
    "        max_nodes=4,\r\n",
    "    )\r\n",
    "\r\n",
    "    # Create the cluster.\r\n",
    "    compute_target = ComputeTarget.create(workspace, cluster_name, compute_config)\r\n",
    "\r\n",
    "    # Can poll for a minimum number of nodes and for a specific timeout.\r\n",
    "    # If no min_node_count is provided, it will use the scale settings for the cluster.\r\n",
    "    compute_target.wait_for_completion(\r\n",
    "        show_output=True, min_node_count=None, timeout_in_minutes=10\r\n",
    "    )\r\n",
    "# For a more detailed view of current AmlCompute status, use get_status().print(compute_target.get_status().serialize())"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1618857535988
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create the expense reports dataset\r\n",
    "\r\n",
    "By creating a dataset, you create a reference to the data source location. If you applied any subsetting transformations to the dataset, they will be stored in the dataset as well. The data remains in its existing location, so no extra storage cost is incurred."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "expenses_datastore = Datastore.get(workspace, datastore_name=\"expense_reports\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build an ML pipeline\r\n",
    "\r\n",
    "### Step 1: data preparation\r\n",
    "\r\n",
    "In step one, we will load the data and labels (expense report amounts) from the Expense Reports dataset and then send it along to the pipeline.\r\n",
    "\r\n",
    "Here is where things get a little tricky.  Our data in the original example was stored in Azure SQL Database, but for pipelines to work, we need to send **mountable** data store information, which means text files in a folder.  We can't send a TabularDataset as an input to a pipeline step, so we'll need to make sure that the data is available in text format in Azure ML.  Instead of pre-loading that data, I'll build the datastore here."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "query = \"\"\"SELECT\r\n",
    "    er.EmployeeID,\r\n",
    "    CONCAT(e.FirstName, ' ', e.LastName) AS EmployeeName,\r\n",
    "    ec.ExpenseCategoryID,\r\n",
    "    ec.ExpenseCategory,\r\n",
    "    er.ExpenseDate,\r\n",
    "    YEAR(er.ExpenseDate) AS ExpenseYear,\r\n",
    "    -- Python requires FLOAT values--it does not support DECIMAL\r\n",
    "    CAST(er.Amount AS FLOAT) AS Amount\r\n",
    "FROM dbo.ExpenseReport er\r\n",
    "    INNER JOIN dbo.ExpenseCategory ec\r\n",
    "        ON er.ExpenseCategoryID = ec.ExpenseCategoryID\r\n",
    "    INNER JOIN dbo.Employee e\r\n",
    "        ON e.EmployeeID = er.EmployeeID\r\n",
    "WHERE\r\n",
    "\tYEAR(er.ExpenseDate) < 2017;\"\"\"\r\n",
    "\r\n",
    "dp = DataPath(expenses_datastore, query)\r\n",
    "\r\n",
    "data_types = {\r\n",
    "    'EmployeeID': DataType.to_long(),\r\n",
    "    'EmployeeName': DataType.to_string(),\r\n",
    "    'ExpenseCategoryID': DataType.to_long(),\r\n",
    "    'ExpenseCategory': DataType.to_string(),\r\n",
    "    'ExpenseDate': DataType.to_datetime('%Y-%m-%d'),\r\n",
    "    'ExpenseYear': DataType.to_long(),\r\n",
    "    'Amount': DataType.to_float()\r\n",
    "}\r\n",
    "\r\n",
    "expensereport_ds = Dataset.Tabular.from_sql_query(dp, set_column_types=data_types).to_pandas_dataframe()"
   ],
   "outputs": [],
   "metadata": {
    "gather": {
     "logged": 1618857756352
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have the dataset and have shaped it as a Pandas dataframe, let's write this out to CSV.  To do that, I'm writing it to a single CSV entitled `ExpenseReports.csv` in the temp folder that we created earlier."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "local_path = tmp_folder + \"/ExpenseReports.csv\"\r\n",
    "expensereport_ds.to_csv(local_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next step is to write the contents of the `tmp` folder to our default datastore, into a folder named `ExpenseReports`.  Once we're done, this data will be in Azure Blob Storage inside a container for Azure ML, with the folder path `ExpenseReports/ExpenseReports.csv`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "datastore = workspace.get_default_datastore()\r\n",
    "datastore.upload(src_dir=tmp_folder, target_path='ExpenseReports')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We already have the datastore object here, so we can retrieve it.  As a quick note, if we already had this data in Azure ML, we could simply reference this `from_delimited_files()` method instead of going through intermediate steps."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "expensereportcsv_ds = Dataset.Tabular.from_delimited_files(datastore.path('ExpenseReports/ExpenseReports.csv'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2: train the model\r\n",
    "\r\n",
    "Our first step in training the model is to specify what our remote compute environment will look like.  There are a few options available to us, but the two most popular methods are to specify a Conda environment and to use a pre-built environment.\r\n",
    "\r\n",
    "Specifying a Conda environment is nice because it provides the most flexibility:  you can install particular versions of libraries and customize what is on that compute machine.  Azure ML will then create a Docker image, include in the `DOCKERFILE` the environment dependencies you specify, and set up a custom machine.  If you want an example of Conda dependencies, check out `src/conda_dependencies.yml` for a simple example.\r\n",
    "\r\n",
    "Another option is to use a pre-built environment.  In this case, we're going to use an environment built into Azure Machine Learning which include `scikit-learn` and everything we will need to perform our model training."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.core import Environment\r\n",
    "\r\n",
    "er_env = Environment.get(workspace=workspace, name=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu\")"
   ],
   "outputs": [],
   "metadata": {
    "gather": {
     "logged": 1618857796171
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, construct a `ScriptRunConfig` to configure the training run that trains a model of expense report expectations. It's going to use the compute target we specified, install the pre-configured environment we specified, and indicate that there is a file named `train.py` in the script folder we specified."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_src = ScriptRunConfig(\r\n",
    "    source_directory=script_folder,\r\n",
    "    script=\"train.py\",\r\n",
    "    compute_target=compute_target,\r\n",
    "    environment=er_env,\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "gather": {
     "logged": 1618857800650
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pass the run configuration details into the PythonScriptStep.\r\n",
    "\r\n",
    "A **PythonScriptStep** is a basic, built-in step to run a Python Script on a compute target. It takes a script name and optionally other parameters like arguments for the script, compute target, inputs and outputs. If no compute target is specified, default compute target for the workspace is used.\r\n",
    "\r\n",
    "Here, we will pass in our flat file expense reports as a prepared dataset and then execute the training script as specified in `train_src` above."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_step = PythonScriptStep(\r\n",
    "    name=\"train step\",\r\n",
    "    arguments=[\r\n",
    "        expensereportcsv_ds.as_named_input(name=\"prepared_expensereport_ds\")\r\n",
    "    ],\r\n",
    "    source_directory=train_src.source_directory,\r\n",
    "    script_name=train_src.script,\r\n",
    "    runconfig=train_src.run_config,\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "gather": {
     "logged": 1618857803377
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build the pipeline\n",
    "Once we have the steps (or steps collection), we can build the pipeline.\n",
    "\n",
    "A pipeline is created with a list of steps and a workspace. Submit a pipeline using `submit`. When submit is called, a PipelineRun is created which in turn creates StepRun objects for each step in the workflow."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# build pipeline & run experiment\r\n",
    "pipeline = Pipeline(workspace, steps=[train_step])\r\n",
    "run = exp.submit(pipeline)"
   ],
   "outputs": [],
   "metadata": {
    "gather": {
     "logged": 1618857811244
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Monitor the PipelineRun"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "run.wait_for_completion(show_output=True)"
   ],
   "outputs": [],
   "metadata": {
    "gather": {
     "logged": 1618858385645
    },
    "inputHidden": false,
    "outputHidden": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "run.find_step_run(\"train step\")[0].get_metrics()"
   ],
   "outputs": [],
   "metadata": {
    "gather": {
     "logged": 1618858387247
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Register the input dataset and the output model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can trace how your data is used in Azure Machine Learning datasets.  Using the `run` object, you can get where and how the datasets are used."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get input datasets\r\n",
    "prep_step = run.find_step_run(\"train step\")[0]\r\n",
    "inputs = prep_step.get_details()[\"inputDatasets\"]\r\n",
    "input_dataset = inputs[0][\"dataset\"]\r\n",
    "\r\n",
    "# list the files referenced by input_dataset\r\n",
    "input_dataset"
   ],
   "outputs": [],
   "metadata": {
    "gather": {
     "logged": 1618858389559
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Register the input Expense Reports dataset with the workspace so that you can reuse it in other experiments or share it with your colleagues who have access to your workspace."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "expensereports_ds = input_dataset.register(\r\n",
    "    workspace=workspace,\r\n",
    "    name=\"expensereports_ds\",\r\n",
    "    description=\"Generated expense report data from 2011-2017\",\r\n",
    "    create_new_version=True,\r\n",
    ")\r\n",
    "expensereports_ds"
   ],
   "outputs": [],
   "metadata": {
    "gather": {
     "logged": 1618858389826
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our last step is to register the output model with dataset.  That way, we can deploy the model as a service."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "run.find_step_run(\"train step\")[0].register_model(\r\n",
    "    model_name=\"ExpenseReportsPipelineModel\",\r\n",
    "    model_path=\"outputs/model.pkl\",\r\n",
    "    datasets=[(\"train test data\", expensereports_ds)],\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "gather": {
     "logged": 1618858392541
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "sihhu"
   }
  ],
  "category": "tutorial",
  "compute": [
   "Remote"
  ],
  "datasets": [
   "Fashion MNIST"
  ],
  "deployment": [
   "None"
  ],
  "exclude_from_index": false,
  "framework": [
   "Azure ML"
  ],
  "friendly_name": "Datasets with ML Pipeline",
  "index_order": 1,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "star_tag": [
   "featured"
  ],
  "tags": [
   "Dataset",
   "Pipeline",
   "Estimator",
   "ScriptRun"
  ],
  "task": "Train",
  "interpreter": {
   "hash": "4905652b14e4b7eb92899b78ac499a22c488804455b27940a322fd82aaf71031"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}